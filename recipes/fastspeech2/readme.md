# fastspeech2利用方法.

- todo
    - hifigan実装
    - finetuning実装
    - synthe実装
    - tts実装

## 事前準備
1. 利用したいコーパスのwavをどこかに置く.
    - srは変更する必要なし.
    - `./run.sh`を実行する場所からの相対パスを指定する.
    - 例: "../../../dataset/out_JSUT/JSUT/wav"
2. 利用したいコーパスのtextgridをどこかに置く.
    - 例: "../../../dataset/out_JSUT/textgrid"
    - textgridのファイルを基準にファイルリストを作成します.

**注意**
- multi-speakerにしたい場合は, ファイル名の前に話者名を「_」を付けて追加すること.
- 例: `JSUT_BASIC5000_0001.wav`

- speakerとstatsに関しては, dump以下に全データ通してのそれをjsonとして吐きだすので, それを参考に入力. 
    - multi-speaker = 0 の時は何も書かなくてよい.

- train/dev/eval splitは毎回shuffleしている. 実験を完全再現したいのであれば, stage 0は実行せず, gitに入っているものをそのまま利用すること.

- 実験名管理について
    - spk: 全体の名前. preprocessed_data名にもなるため, データそのものの変更をするならここを変える.
    - tag: 毎度おなじみexp_name. 実験名の変更です. 訓練時.


## memo
- preprocess.pyにおいて, 今回のように複数の特徴量を出したい場合は, フォルダの方で管理. ファイル名自体は, `ファイル名-feats.npy`を遵守. そうでないとfitscalerなどを使えない.
    - 複数特徴量を使いたい場合, 選択肢はおそらく2つ.
        - 1つのファイルにまとめる
            - こうすることで, テンプレ通りの処理が可能になるが, 今回は難しそう.
            なぜなら, durationは正規化したくないから.
        - 複数ファイルをあきらめて作る
            - この場合, その数だけフォルダができる.
            - その分, datasetの使いまわしができない等の問題が発生してしまう.
            - 基本の型は同じでいいが, 一部修正する必要あり. すべてcollate_fn内で.
            - set_upの時に, originalのget_dataloaderを渡せば終わり.

- preprocess.pyにて, speaker.json, stats.jsonの計算.
    - eval, devを跨げるようにロード機能をつけることを考えたが, 
    statsはtrainだけで計算するべきなきがする.
    なので, 実装的に面倒なので, statsは各train/dev/evalで作成し, 実行時に用いるのはtrainのものということにする.
    - ↑ そもそも, modelで使うものは, すべてyamlに書き込んでいないとだめ.
    なので, やり方が間違っていて, confに直接書き込んでしまうのがよい.

    - confに直接書き込むのも問題あり. パスを動的にできないし.
    - そもそも, confは動的に書くべきではない.
        - pitchを動的に決めてモデルに入れているのがまず問題.
        - 決め打ちでいけ.

- 上位のconfigで決めたところは空白にしておく. そうでないと二重に設定することになりバグの温床.
