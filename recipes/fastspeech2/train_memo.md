# train_memo
今まで同様に, trainの詳細をここに記載しておく.

- spk
    - jsut
        - 先生にアライメントも二回目の修正をしてもらったきれいなやつ.
    - Long_dialogue
        - 先生にもらったもので, soxで22050にしたもの.
    - JSUT_accent
        - tacotronの実装にある, フルコンテキストラベルのアクセント情報を利用したデータセット
    - LINE
        - corpus評価用のwoITAKO.
    - LINE_2
        - corpus評価用のwoITAKO.
        - statsの計算方法を変えたので, 一応作り直し.
    - LINE_3
        - corpus評価用のwoITAKO.
        - accent info付き.
    - LINE_4
        - corpus評価用のwoITAKO_before_emotion.
        - accent info付き.
        - emotionを変えたのでdataをコピーしなおしていることに注意
    
    - JSUT_NICT_LINE
        - VCのpretrain用の大規模TTS. textgrid.
    
    - LINE_Teacher
        - wContexts用に, ProsodyExtractorを学習するためのコーパス
        - +accent info, +ITAKO
    - LINE_MStudent
        - 同様
    - LINE_FStudent
        - 同様

- exp
    - jsut_1
        - Long_dialogueのための, pretrain.
    - Long_dialogue_1
        - emotionを実装してからの初実行. 比較対象がないのが残念.
    - Long_dialogue_2
        - emotionを外して. Long_dialogue_1と全く同じtrain_set.
        - さらに, hifiganも完全に使いまわし.
    - Long_dialogue_3
        - pretrainだけなしにして, それ以外は2と同じ.
        - lossはほぼ同じだけど, 音質は若干悪い感じ.
    - Long_dialogue_4
        - spk: Long_dialogue
        - pretrain: None
        - wGMM初稼働. ミスって上書きしてしまった...。
    - Long_dialogue_5
        - spk: Long_dialogue
        - pretrain: None
        - wGMM実質初稼働. ミスって上書きしてしまった...。
    - Long_dialogue_6
        - spk: Long_dialogue
        - pretrain: None
        - wGMM+global prosody.
    - Long_dialogue_7
        - spk: Long_dialogue
        - pretrain: None
        - optunaでハイパラ探索を行う
    - JSUT_accent_1(@myPC)
        - spk: JSUT_accent
        - pretrain: None
        - アクセント付きFastSpeech2初実行
    - Long_dialogue_8
        - spk: Long_dialogue
        - pretrain: None
        - optunaでハイパラ探索を行う. 別パラメタ。アドバイスを受けて大分変数を減らした.
    - Long_dialogue_9
        - spk: Long_dialogue
        - pretrain: None
        - optunaでハイパラ探索を行う. 別パラメタ。
            - prunerを変えてnum_gaussianをカテゴリ―にした.
    
    - ここで、VCの方にシフト. lr_scheduler周りで2つほどミスを見つけたりの修正が出来たので、ましになるかも.
    - wGMMも他のも, アクセントを前提にしてみる.
    - LINE_1
        - spk: LINE
        - pretrain: None
        - ふっつーのfastspeech2.
    - LINE_2
        - spk: LINE_2
        - pretrain: None
        - 29min/50epoch
        - batch_size: 8
        - ふっつーのfastspeech2. LINE_1でstatsが正規化前のものになっているのに気づかず使っていた. そこを修正して実行しなおしたもの.
    - LINE_3
        - spk: LINE_3
        - pretrain: None
        - +accent info
        - batch_size: 8
        - loss上は微改善. 音声も劇的ではないが、改善.
    - LINE_4
        - spk: LINE_3
        - pretrain: None
        - +accent info + emotion
        - batch_size: 8
    - LINE_5
        - spk: LINE_4
        - pretrain: None
        - +accent info + before_emotion
        - batch_size: 8
    
    - LINE_Teacher_1
        - spk: LINE_Teacher
        - pretrain: None
    
    - 以下は趣味のVC用.
    - JSUT_NICT_LINE_1
        - spk: JSUT_NICT_LINE
        - pretrain用.
    
    - 以下は自分の研究用
    - LINE_6
        - spk: LINE_3
        - pretrain: None
        - +accent info
        - batch_size: 32
        - 500epoch
        - wGMM. baeslineとして、どの程度の精度が出るのかを確認するために回してみる。
    - LINE_6(hifigan)
        - spk: LINE_3
        - pretrain: None
        - batch_size: 128
        - 1000epoch
        - ちゃんと論文に出せるように、Universal重みからスタートして、train.listを使ってhifiganを訓練してみる.
    - LINE_7
        - spk: LINE_3
        - pretrain: None
        - +accent info
        - batch_size: 16
        - 500epoch
        - wGMM. baeslineとして、どの程度の精度が出るのかを確認するために回してみる。
        - 比較できるように, batchを16にして実行.
    - LINE_8
        - spk: LINE_3
        - pretrain: None
        - +accent info
        - batch_size: 16
        - 500epoch
        - fastspeech2. 比較用としてbatch_size=16として再実行
    - LINE_9
        - spk: LINE_3
        - pretrain: None
        - +accent info
        - batch_size: 16
        - 500epoch
        - fastspeech2wGMM. 
        - global prosody: False, local prosody: True
        - つまり、元論文の設定.
        - job_ID: 8221619
    - LINE_10
        - spk: LINE_3
        - pretrain: None
        - +accent info
        - batch_size: 16
        - 500epoch
        - fastspeech2wGMM. 
        - global prosody: True, local prosody: False
        - つまり、globalのみ使ってみよう作戦
        - job_ID: 8221622
    - LINE_11
        - spk: LINE_3
        - pretrain: None
        - +accent info
        - batch_size: 16
        - 500epoch
        - fastspeech2wGMM. 
        - global prosody: True, local prosody: True, g_beta: 0.0
        - つまり、globalのlossは計算しない.
        - job_ID: 8221685
    - LINE_12
        - spk: LINE_3
        - pretrain: LINE_10(250epochから)
        - +accent info
        - batch_size: 16
        - 250epoch
        - fastspeech2wGMM. 
        - global prosody: True, local prosody: True, g_beta: 0.02, beta: 0.02
        - まず最初にglobal prosodyのみ訓練して、途中からlocalも.
        - 公平になるように250, 250でやる.
        - job_ID: 8223475
    - LINE_13
        - spk: LINE_3
        - pretrain: None
        - +accent info
        - batch_size: 16
        - 500epoch
        - fastspeech2wGMM. 
        - global prosody: True, local prosody: True, g_beta: 0.02, beta: 0.02, max_seq_len: 4000
        - 基本今までの設定、つまりLINE_7と同じだが、max_seq_lenを1000にしていたせいで、ろくにデータを使えていなかったことが発覚.
        - 4倍に増やしてみる.
        - job_ID: 8226280
    - LINE_14
        - spk: LINE_3
        - pretrain: None
        - +accent info
        - batch_size: 16
        - 500epoch
        - fastspeech2wGMM. 
        - global prosody: True, local prosody: False, g_beta: 0.02, beta: 0.02, max_seq_len: 4000
        - LINE_10の, max_seq_len増やしたver. pretrain用.
        - job_ID: 8226389
    - LINE_15
        - spk: LINE_3
        - pretrain: LINE_14(250epoch)
        - +accent info
        - batch_size: 16
        - 250epoch
        - fastspeech2wGMM. 
        - global prosody: True, local prosody: True, g_beta: 0.02, beta: 0.02, max_seq_len: 4000
        - LINE_12のmax_seq_len増やしたver.
        - job_ID: 

## 主要な実験

## 知見