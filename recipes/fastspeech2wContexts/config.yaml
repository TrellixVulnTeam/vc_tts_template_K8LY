# General settings.
spk: "LINE_wContextwPEProsody_2"

# exp tag(for managing experiments)
tag: "LINE_emotionprediction_10_CE_PEPCE_woCurrent"

sample_rate: 22050

# 1) none 2) tqdm
# NOTE: Jupyterノートブックからrun.shを実行する場合は、none推奨
tqdm: tqdm

# NOTE: benchmarkをtrueにすると、高速化が期待できる分、より多くの
# GPUリソースを必要とする場合があります。
# GPUリソースに余裕がある場合は、true にしてください。
cudnn_benchmark: false
cudnn_deterministic: false

###########################################################
#                DATA PREPARATION SETTING                 #
###########################################################

# PLEASE CHANGE THE PATH BASED ON YOUR ENVIRONMENT
wav_root: "../../../dataset/out_LINE_woITAKO/LINE/wav/22050"
lab_root: "../../../dataset/out_LINE_woITAKO/fullcontext"
dialogue_info: "../../../dataset/out_LINE_woITAKO/dialogue_info.txt"

n_jobs: 8

train_num:
deveval_num:
dev_num:
eval_num:

###########################################################
#                FEATURE EXTRACTION SETTING               #
###########################################################
filter_length: 1024
hop_length: 256
win_length: 1024
n_mel_channels: 80
mel_fmin: 0
mel_fmax: 8000
clip: 0.00001
log_base: "natural"
pitch_phoneme_averaging: 1
energy_phoneme_averaging: 1
accent_info: 1
# if you want to use multi-speaker, fill below.
# example: "JSUT,NICT"
speakers:

# for with Contexts
BERT_weight: "colorfulscoop/sbert-base-ja"
use_hist_num: 10
# for with Prosody
# if 0, the latest one is used. if -1, data will not be loaded.
use_local_prosody_hist_idx: -1
# plase replace train/dev/eval to {} for emb_preprocess.py
## もし, GMMを学習して作成するembを利用したい場合は以下を埋める
input_duration_paths:
output_mel_file_paths:
model_config_paths:
pretrained_checkpoints:
## そうではなく, pitch, energyを利用したい場合は以下を埋める
## labかtextgridは片方を埋めればよい
input_wav_paths: "../../../dataset/out_LINE_woITAKO/LINE/wav/22050,../../../dataset/out_LINE_MStudent/LINE_MStudent/wav/22050,../../../dataset/out_LINE_FStudent/LINE_FStudent/wav/22050"
input_lab_paths: "../../../dataset/out_LINE_woITAKO/fullcontext,../../../dataset/out_LINE_MStudent/fullcontext,../../../dataset/out_LINE_FStudent/fullcontext"
input_textgrid_paths:
emb_speakers: "Teacher,MStudent,FStudent"

###########################################################
#                TRAINING SETTING                         #
###########################################################

acoustic_model: EmotionPredictor
vocoder_model: hifigan
# acoustic_modelで利用したいvocoderのconfigやweightへのpathを指定してください.
# 具体的に利用する重みは,vocoder_eval_checkpointになります.
vocoder_config: "conf/train_hifigan/model/hifigan.yaml"
vocoder_weight_base_path: "../fastspeech2/exp/LINE_3_sr22050_LINE_6/hifigan"

### fastspeech2  ###
# max_train_steps: 200000 → nepochs: 256 s.t. batch_size*group_size = 32, JSUT.
fastspeech2_train_nepochs: 500
fastspeech2_data_batch_size: 16

### hifigan ###
hifigan_train_nepochs: 50
hifigan_data_batch_size: 32

### (optional) Optuna Tuning ###
tuning_config: tuning_fastspeech2wContexts

###########################################################
#                SYNTHESIS SETTING                        #
###########################################################

# リストの逆順で発話を処理する
reverse: false

# 生成する発話の数
# -1 の場合、評価の発話をすべて処理する
# 音声生成にかかる時間を短縮する場合、小さな値（5など）に設定してください
num_eval_utts: -1

acoustic_eval_checkpoint: latest.pth
vocoder_eval_checkpoint: latest.pth
